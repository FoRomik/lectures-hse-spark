{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создайте ключи для доступа к потоку на сайте https://apps.twitter.com\n",
    "# Запустите прокси ./data/twitter.py для чтения потока твитов\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# создаем локальный контекст с двумя потоками\n",
    "sc = SparkContext(master='local[2]', appName='Twitter Processing')\n",
    "\n",
    "# создаем потоковый контескт\n",
    "streaming = StreamingContext(sc, batchDuration=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Читаем данные и считаем статистики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# подключаемся к потоку твитов и загружаем данные в json формате\n",
    "tweet_ds = streaming.socketTextStream(\n",
    "             hostname='localhost', port=8889) \\\n",
    "                    .map(json.loads)\n",
    "\n",
    "# создаем окно 30сек, обновляем даные каждые 5сек\n",
    "tweet_window_ds = tweet_ds.window(30, 5)\n",
    "    \n",
    "# разбиваем текст на слова\n",
    "words_ds = tweet_window_ds.map(lambda entry: entry['text']) \\\n",
    "              .flatMap(lambda line: line.lower().split())\\\n",
    "              .filter(lambda word: word.startswith('#'))\n",
    "\n",
    "# считаем частоты слов\n",
    "counts_ds = words_ds.map(lambda word: (word, 1)) \\\n",
    "              .reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "# сортируем слова по убыванию частоты \n",
    "sorted_ds = counts_ds.transform(\n",
    "  lambda rdd: rdd.sortBy(lambda item: -item[1]))\n",
    "\n",
    "# выводим сортированый список\n",
    "# sorted_ds.pprint(num=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# запускаем чтение данных из потока\n",
    "# streaming.start()\n",
    "\n",
    "# продолжаем чтение до тех пор, пока не произойдет прерывание выполнения\n",
    "# streaming.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Строим модель оценки тональности хештега"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загружаем данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# загружаем данные для обучения модели\n",
    "sentiment_rdd = sc.textFile('../data/sentiment_sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# контекст для создания DataFrame\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "def process_line(line): \n",
    "    # оставляем пробелы и латинские символы\n",
    "    line = ''.join(char for char in line\n",
    "                   if char in string.ascii_letters \n",
    "                   or char == ' ')\n",
    "    return line.lower().split()\n",
    "\n",
    "# создаем DataFrame\n",
    "sentiment_df = sentiment_rdd\\\n",
    "        .map(lambda line: line.split(','))\\\n",
    "        .map(lambda row: Row(label=int(row[1]), \n",
    "             text=process_line(row[3]))).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|                text|\n",
      "+-----+--------------------+\n",
      "|    0|[is, so, sad, for...|\n",
      "|    0|[i, missed, the, ...|\n",
      "|    1|[omg, its, alread...|\n",
      "+-----+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentiment_df.show(3, truncate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Препроцессинг и построение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import NGram\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.ml.feature import HashingTF\n",
    "from pyspark.ml import Pipeline \n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "with open('../../004/data/stopwords.txt') as src:\n",
    "    stopwords_list = [word.strip() for word in src]\n",
    "    \n",
    "stopwords = StopWordsRemover(inputCol='text', \n",
    "                             outputCol='stopwords',\n",
    "                             stopWords=stopwords_list)\n",
    "\n",
    "ngram = NGram(n=2, \n",
    "              inputCol=stopwords.getOutputCol(),\n",
    "              outputCol='ngram')\n",
    "\n",
    "hashing = HashingTF(numFeatures=4096,\n",
    "                    binary=True,\n",
    "                    inputCol=ngram.getOutputCol(),\n",
    "                    outputCol='hashing')\n",
    "\n",
    "logreg = LogisticRegression(\n",
    "    featuresCol=hashing.getOutputCol(),\n",
    "    labelCol='label',\n",
    "    regParam=1e-1)\n",
    "\n",
    "# пайплайн обработки текста\n",
    "pipeline = Pipeline(stages=[stopwords, \n",
    "                            ngram,\n",
    "                            hashing,\n",
    "                            logreg])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подбираем параметры модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import TrainValidationSplit\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "\n",
    "params = ParamGridBuilder()\\\n",
    "   .addGrid(hashing.numFeatures, [4096, 8 * 4096])\\\n",
    "   .addGrid(logreg.regParam, [1e-3, 1e-2, 1e-1])\\\n",
    "   .build()\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(\n",
    "                  metricName='areaUnderROC')\n",
    "\n",
    "split = TrainValidationSplit(estimator=pipeline,\n",
    "                             evaluator=evaluator,\n",
    "                             seed=12345,\n",
    "                             estimatorParamMaps=params)\n",
    "\n",
    "eval_result = split.fit(sentiment_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>areaUnderROC</th>\n",
       "      <th>numFeatures</th>\n",
       "      <th>regParam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.612853</td>\n",
       "      <td>4096</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.612848</td>\n",
       "      <td>4096</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.612726</td>\n",
       "      <td>4096</td>\n",
       "      <td>0.100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.674075</td>\n",
       "      <td>32768</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.674281</td>\n",
       "      <td>32768</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.675329</td>\n",
       "      <td>32768</td>\n",
       "      <td>0.100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   areaUnderROC  numFeatures  regParam\n",
       "0      0.612853         4096     0.001\n",
       "1      0.612848         4096     0.010\n",
       "2      0.612726         4096     0.100\n",
       "3      0.674075        32768     0.001\n",
       "4      0.674281        32768     0.010\n",
       "5      0.675329        32768     0.100"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "pd.DataFrame([dict([(k.name, v) for k, v in param.items()]\n",
    "                   + [(evaluator.getMetricName(), metric)])\n",
    "              for param, metric in zip(params, eval_result.validationMetrics)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# сохраняем модель с лучшим результатом\n",
    "eval_result.bestModel.save('logreg.spark')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Применяем модель на потоковых данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "# загружаем обученную модель (пайплайн)\n",
    "model = PipelineModel.load('logreg.spark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# контекст для создания DataFrame\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "def transform_rdd(rdd):  \n",
    "    # подсчитываем среднее значение тональности хештега\n",
    "    \n",
    "    def _get_hashtags(line):\n",
    "        return [word for word in line.lower().split()\n",
    "                if word.startswith('#')]\n",
    "    \n",
    "    # проверяем не пустой ли rdd\n",
    "    if len(rdd.take(1)) == 0:\n",
    "        return rdd\n",
    "    \n",
    "    # создаем DataFrame для получения предсказаний модели\n",
    "    df = rdd.map(lambda entry: Row(\n",
    "                 text=process_line(entry['text']), \n",
    "                 hashtags=_get_hashtags(entry['text']))).toDF()\n",
    "    \n",
    "    # получаем предсказания\n",
    "    prediction_df = model.transform(df)\n",
    "    \n",
    "    hashtags_rdd = prediction_df.rdd\\\n",
    "            .flatMap(lambda row: \n",
    "                     [(hashtag, (1, row.probability[1])) \n",
    "                      for hashtag in row.hashtags])\n",
    "    \n",
    "    # получаем усредненную оценку тональности хештега\n",
    "    result = hashtags_rdd.reduceByKey(\n",
    "         lambda x, y: (x[0] + y[0], x[1] + y[1]))\\\n",
    "          .filter(lambda k_v: k_v[1][0] > 3)\\\n",
    "          .map(lambda k_v: (k_v[0], k_v[1][1] / k_v[1][0]))\n",
    "\n",
    "    return result\n",
    "\n",
    "# оцениваем тональность хештега в окне 30сек\n",
    "tweet_window_ds.transform(transform_rdd).pprint(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# запускаем чтение данных из потока\n",
    "streaming.start()\n",
    "\n",
    "# продолжаем чтение до тех пор, пока не произойдет прерывание выполнения\n",
    "streaming.awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
