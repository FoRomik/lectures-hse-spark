{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# init SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "sc = SparkContext('local[4]', 'Pipeline')\n",
    "spark = SparkSession.builder.appName('Pipeline').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!head ../data/reviews_sample.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews: 5000\n",
      "+-------+--------------------+\n",
      "|overall|          reviewText|\n",
      "+-------+--------------------+\n",
      "|    4.0|It fits and repla...|\n",
      "|    5.0|I own several Ank...|\n",
      "|    5.0|Love these kind o...|\n",
      "|    5.0|This is the best ...|\n",
      "|    5.0|Great collection ...|\n",
      "+-------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load reviews fro json file\n",
    "data_frame = spark.read.json('../data/reviews_sample.json') \\\n",
    "                       .select('overall', 'reviewText')\n",
    "\n",
    "print('Number of reviews: %d' % data_frame.count())\n",
    "data_frame.show(5, truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|overall|          reviewText|\n",
      "+-------+--------------------+\n",
      "|    3.0|it fits and repla...|\n",
      "|    4.0|i own several ank...|\n",
      "|    4.0|love these kind o...|\n",
      "|    4.0|this is the best ...|\n",
      "|    4.0|great collection ...|\n",
      "+-------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: convert lines to lower case using data_frame.rdd.map transformation\n",
    "from pyspark.sql import Row\n",
    "\n",
    "lower_rdd = data_frame.rdd.map(\n",
    "                    lambda row: Row(\n",
    "                        overall=row.overall - 1, \n",
    "                        reviewText=row.reviewText.lower()))\n",
    "\n",
    "data_frame_lower = spark.createDataFrame(lower_rdd)\n",
    "\n",
    "data_frame_lower.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+\n",
      "|overall|count(overall)|\n",
      "+-------+--------------+\n",
      "|    0.0|           356|\n",
      "|    1.0|           292|\n",
      "|    4.0|          2815|\n",
      "|    3.0|          1003|\n",
      "|    2.0|           534|\n",
      "+-------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "data_frame_lower.groupBy('overall').agg(count('overall')).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.ml.feature import NGram\n",
    "from pyspark.ml.feature import HashingTF\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "tokenizer = Tokenizer(inputCol='reviewText',\n",
    "                      outputCol='tokenized')\n",
    "\n",
    "with open('../data/stopwords.txt') as src:\n",
    "    stopwords_list = list(map(str.strip, src))\n",
    "    \n",
    "stopwords = StopWordsRemover(\n",
    "    stopWords=stopwords_list,\n",
    "    inputCol=tokenizer.getOutputCol(),\n",
    "    outputCol='stopwords')\n",
    "\n",
    "ngram = NGram(n=2, inputCol=stopwords.getOutputCol(),\n",
    "                 outputCol='ngram')\n",
    "\n",
    "hashing = HashingTF(numFeatures=1024, \n",
    "                    binary=True, \n",
    "                    inputCol=ngram.getOutputCol(),\n",
    "                    outputCol='hashing')\n",
    "\n",
    "logreg = LogisticRegression(featuresCol=hashing.getOutputCol(),\n",
    "                    labelCol='overall',\n",
    "                    family='multinomial',\n",
    "                    regParam=1e-3)\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer,\n",
    "                            stopwords, \n",
    "                            ngram,\n",
    "                            hashing,\n",
    "                            logreg])\n",
    "\n",
    "model = pipeline.fit(data_frame_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: get pipeline prediction using transform()\n",
    "pred_df = model.transform(data_frame_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+--------------------+\n",
      "|overall|prediction|          reviewText|\n",
      "+-------+----------+--------------------+\n",
      "|    3.0|       4.0|it fits and repla...|\n",
      "|    4.0|       2.0|love these kind o...|\n",
      "|    3.0|       3.0|i recently receiv...|\n",
      "|    4.0|       4.0|fits just as well...|\n",
      "|    4.0|       4.0|this little guy i...|\n",
      "|    4.0|       4.0|slim, light, grea...|\n",
      "|    4.0|       4.0|this replaced my ...|\n",
      "|    0.0|       4.0|i wouldn't recomm...|\n",
      "|    3.0|       4.0|the three items h...|\n",
      "|    3.0|       4.0|you can always us...|\n",
      "|    4.0|       4.0|i read other revi...|\n",
      "|    2.0|       2.0|i purchased the h...|\n",
      "|    3.0|       3.0|as others have sa...|\n",
      "|    4.0|       4.0|this case is beau...|\n",
      "|    4.0|       4.0|this product hold...|\n",
      "|    2.0|       2.0|but i passed this...|\n",
      "|    1.0|       1.0|overall seems a g...|\n",
      "|    4.0|       4.0|these work great ...|\n",
      "|    3.0|       3.0|i received my blu...|\n",
      "|    2.0|       4.0|since you have th...|\n",
      "+-------+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_df.select(['overall', 'prediction', 'reviewText'])\\\n",
    "       .sample(False, .25).show(20, truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.90      0.71      0.79       356\n",
      "        1.0       0.93      0.80      0.86       292\n",
      "        2.0       0.74      0.49      0.59       534\n",
      "        3.0       0.69      0.47      0.56      1003\n",
      "        4.0       0.74      0.91      0.82      2815\n",
      "\n",
      "avg / total       0.76      0.75      0.74      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# TODO: estimate quality\n",
    "\n",
    "target = np.array(pred_df.select(['overall']).collect())\n",
    "predicted = np.array(pred_df.select(['prediction']).collect())\n",
    "\n",
    "acc = accuracy_score(target, predicted)\n",
    "report = classification_report(target, predicted)\n",
    "\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# TODO: build param grid\n",
    "# TODO: find best param match using accuracy as a target metric\n",
    "\n",
    "params = ParamGridBuilder()\\\n",
    "  .addGrid(hashing.numFeatures, [128, 512, 1024]) \\\n",
    "  .addGrid(logreg.regParam, [1e-1, 1e-3])\\\n",
    "  .build()\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    metricName='accuracy', \n",
    "    labelCol='overall')\n",
    "\n",
    "cv = CrossValidator(estimator=pipeline, \n",
    "                    estimatorParamMaps=params,\n",
    "                    evaluator=evaluator)\n",
    "\n",
    "crossval_model = cv.fit(data_frame_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('numFeatures', 128), ('regParam', 0.1)] 0.5572597844490846\n",
      "[('numFeatures', 128), ('regParam', 0.001)] 0.5409111962743384\n",
      "[('numFeatures', 512), ('regParam', 0.1)] 0.5355030436548964\n",
      "[('numFeatures', 512), ('regParam', 0.001)] 0.4266091768797028\n",
      "[('numFeatures', 1024), ('regParam', 0.1)] 0.5186645530270984\n",
      "[('numFeatures', 1024), ('regParam', 0.001)] 0.36458632906686267\n"
     ]
    }
   ],
   "source": [
    "# output average metric for each param set\n",
    "for accuracy, params in zip(crossval_model.avgMetrics, \n",
    "                            cv.getEstimatorParamMaps()):\n",
    "    params = [(p.name, v) for p, v in params.items()]\n",
    "    print(params, accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
